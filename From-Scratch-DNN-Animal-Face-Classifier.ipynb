{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 배열 및 행렬 연산을 사용하고자 수치 연산을 위한 numpy라이브러리를 사용하였습니다.\n",
    "- 운영체제와 상호작용하기 위한 모듈로, 파일 및 디렉토리 경로 조작에 사용되는 os라이브러리를 사용했습니다\n",
    "- 그래프와 시각화를 위해 matplotlib.pyplot 라이브러리를 사용했습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 상수 정의\n",
    "NUM_CLASSES = 4\n",
    "BATCH_SIZE = 64\n",
    "INITIAL_LEARNING_RATE = 0.001\n",
    "EPOCHS = 60\n",
    "INPUT_NODES = 256 * 256\n",
    "HIDDEN_LAYERS = 6\n",
    "HIDDEN_NODES = [128,128,128,64,64,64]\n",
    "OUTPUT_NODES = NUM_CLASSES\n",
    "L2_LAMBDA = 0.001\n",
    "EPSILON = 1e-5 \n",
    "\n",
    "# 그레이디언트 클리핑 임계값\n",
    "MAX_GRAD_NORM = 8.0\n",
    "\n",
    "# 클래스 라벨\n",
    "animal_names = [\"cat\", \"dog\", \"tiger\", \"hyena\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- NUM_CLASSES: 분류할 객체의 수로, 4개의 동물 클래스입니다.\n",
    "- BATCH_SIZE: 한 번의 학습 단계에서 사용할 데이터 샘플의 수로 32로 설정했습니다.\n",
    "- INITIAL_LEARNING_RATE: 초기 학습률입니다.\n",
    "- EPOCHS: 전체 데이터셋을 몇 번 반복하여 학습할 것인지 결정하는 에폭입니다.\n",
    "- INPUT_NODES: 입력층의 노드 수로, 신경망의 입력층에 전달되는 데이터의 크기, 이미지의 픽셀 수입니다.\n",
    "- HIDDEN_LAYERS: 은닉층의 수입니다.\n",
    "- HIDDEN_NODES: 각 은닉층의 노드 수를 리스트로 정의하였습니다.\n",
    "- OUTPUT_NODES: 출력층의 노드 수로 4개의 클래스 중 하나로 분류해야 하기에 다음과 같이 설정하였다.\n",
    "- L2_LAMBDA: L2 정규화의 람다 값으로, 과적합을 방지하기 위해 사용하였습니다.\n",
    "- EPSILON: 작은 값으로, 계산 중에 0으로 나누는 오류를 방지하고자 사용하였습니다.\n",
    "- MAX_GRAD_NORM: 그레이디언트 클리핑을 위한 임계값으로, 그레이디언트 폭주를 방지하고자 사용하였습니다.\n",
    "- animal_names: 클래스 라벨을 정의한 리스트입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 데이터 로드 함수 정의\n",
    "def load_pgm_image(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        file.readline()  # \"P5\"\n",
    "        file.readline()  # width height\n",
    "        file.readline()  # max_val\n",
    "        data = np.fromfile(file, dtype=np.uint8)\n",
    "        data = data.reshape((256, 256)).astype(np.float32) / 255.0\n",
    "    return data.flatten()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 데이터셋의 이미지들이 pgm형식으로 되어있는데, pgm 이미지 파일을 읽어와서 일차원 배열로 반환하는 함수입니다**.**\n",
    "- 바이너리 모드로 파일을 열고, 헤더 부분(\"P5\", 이미지 크기, 최대 값)을 읽어 넘깁니다.\n",
    "- 픽셀 데이터를 np.uint8 타입으로 읽어옵니다.\n",
    "- 이미지를 (256, 256) 형태로 재구성하고, float32로 변환 후 0~1 사이로 정규화합니다.\n",
    "- 이미지를 평탄화하여 일차원 배열로 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_images_with_labels(folder_path, filenames):\n",
    "    data = []\n",
    "    labels = []\n",
    "    for filename in filenames:\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        data.append(load_pgm_image(file_path))\n",
    "        label = get_label_from_filename(filename)\n",
    "        if label == -1:\n",
    "            print(f\"Warning: Label not found for file {filename}\")\n",
    "        labels.append(label)\n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "def get_label_from_filename(filename):\n",
    "    filename = filename.lower() \n",
    "    for i, name in enumerate(animal_names):\n",
    "        if name in filename:\n",
    "            return i\n",
    "    print(f\"Warning: Label not found for file {filename}\")\n",
    "    return -1  # 라벨을 찾지 못한 경우\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 데이터셋의 이미지들이 pgm형식으로 되어있는데, pgm 이미지 파일을 읽어와서 일차원 배열로 반환하는 함수입니다**.**\n",
    "- 바이너리 모드로 파일을 열고, 헤더 부분(\"P5\", 이미지 크기, 최대 값)을 읽어 넘깁니다.\n",
    "- 픽셀 데이터를 np.uint8 타입으로 읽어옵니다.\n",
    "- 이미지를 (256, 256) 형태로 재구성하고, float32로 변환 후 0~1 사이로 정규화합니다.\n",
    "- 이미지를 평탄화하여 일차원 배열로 반환합니다.\n",
    "\n",
    "- 파일명에서 클래스 라벨을 추출하는 함수입니다.\n",
    "- 데이터셋 증강을 위해 이미지들을 조정하다보니 이미지의 이름들에서 class 말고 cat_24_rot-15 이런식으로 되어있기에 클래스 라벨을 추출할 필요성을 느껴서 추가하였습니다.\n",
    "- 이미지들 이름에서 클래스들이 전부 소문자로 되어있기에 파일명을 소문자로 변환하여 일관성을 유지하고자 하였습니다.\n",
    "- animal_names 리스트를 순회하며 파일명에 해당 동물명이 포함되어 있는지 확인합니다.\n",
    "- 해당 동물명이 포함되어 있으면 그 인덱스를 라벨로 반환합니다.\n",
    "- 해당되지 않는 경우 -1을 반환하고 경고 메시지를 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 활성화 함수 정의\n",
    "def leaky_relu(x, alpha=0.0001):\n",
    "    return np.where(x > 0, x, alpha * x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Leaky ReLU 활성화 함수를 구현하였습니다.\n",
    "- 입력 x가 양수이면 그대로 반환하고, 음수이면 alpha를 곱하여 기울기를 유지합니다.\n",
    "- alpha는 음수 영역의 기울기로 기본값은 0.01로 설정하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def leaky_relu_derivative(x, alpha=0.0001):\n",
    "    dx = np.ones_like(x)\n",
    "    dx[x < 0] = alpha\n",
    "    return dx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Leaky ReLU의 도함수를 계산하는 함수로 역전파 시 그레이디언트 계산에 사용됩니다.\n",
    "- 입력 x가 양수이면 도함수는 1, 음수이면 alpha입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# He 초기화 함수\n",
    "def he_initialize(rows, cols):\n",
    "    return np.random.randn(rows, cols) * np.sqrt(2.0 / rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 가중치를 초기화 하는 함수로 He 초기화 함수를 선택하였습니다. 깊은 신경망에서의 기울기 소실 문제를 완화할 수 있는 역할을 합니다.\n",
    "- 표준 정규분포에서 난수를 생성하고, 분산을 2.0 / rows로 조정합니다.\n",
    "- rows는 입력 노드 수, cols는 출력 노드 수입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 네트워크 초기화 함수\n",
    "def init_network():\n",
    "    weights = []\n",
    "    biases = []\n",
    "\n",
    "    for i in range(HIDDEN_LAYERS):\n",
    "        in_nodes = INPUT_NODES if i == 0 else HIDDEN_NODES[i - 1]\n",
    "        out_nodes = HIDDEN_NODES[i]\n",
    "        weights.append(he_initialize(in_nodes, out_nodes))\n",
    "        biases.append(np.zeros((1, out_nodes)))\n",
    "    \n",
    "    # 출력층\n",
    "    weights.append(he_initialize(HIDDEN_NODES[-1], OUTPUT_NODES))\n",
    "    biases.append(np.zeros((1, OUTPUT_NODES)))\n",
    "    return weights, biases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 신경망의 가중치와 편향을 초기화하는 함수입니다.\n",
    "- 은닉층의 수만큼 루프를 돌며 각 층의 가중치와 편향을 초기화합니다.\n",
    "- 첫 번째 층의 입력 노드 수는 INPUT_NODES이며, 그 이후 층은 이전 층의 노드 수를 사용합니다.\n",
    "- 가중치는 he_initialize 함수를 사용하여 초기화하고, 편향은 0으로 초기화합니다.\n",
    "- 출력층의 가중치와 편향도 동일하게 초기화합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 순전파 함수\n",
    "def forward(weights, biases, input_data):\n",
    "    caches = []\n",
    "    out = input_data\n",
    "\n",
    "    for i in range(HIDDEN_LAYERS):\n",
    "        z = out @ weights[i] + biases[i]\n",
    "        a = leaky_relu(z)\n",
    "        caches.append((out, weights[i], biases[i], z))\n",
    "        out = a\n",
    "\n",
    "    # 출력층\n",
    "    z = out @ weights[-1] + biases[-1]\n",
    "    z_exp = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "    softmax_output = z_exp / np.sum(z_exp, axis=1, keepdims=True)\n",
    "    caches.append((out, weights[-1], biases[-1], None))\n",
    "    return softmax_output, caches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 순전파를 수행하여 입력 데이터로부터 예측값을 계산하는 함수입니다.\n",
    "- caches 리스트는 역전파 시 필요한 중간 계산 값을 저장합니다.{효율적인 학습(역전파)을 위해}\n",
    "- 각 은닉층에 대해:\n",
    "    - z는 선형 변환 결과이며, z = out @ weights[i] + biases[i]로 계산됩니다.\n",
    "    - a는 활성화 함수를 적용한 결과입니다.\n",
    "    - caches에 입력, 가중치, 편향, z를 저장합니다.\n",
    "    - out은 다음 층의 입력이 됩니다.\n",
    "- 출력층에서:\n",
    "    - 선형 변환 후 softmax 함수를 적용하여 클래스 확률을 계산합니다.\n",
    "    - 안정성을 위해 z에서 최대값을 빼줍니다 (Overflow 방지).\n",
    "- 최종적으로 예측값과 caches를 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 손실 함수 (Cross Entropy Loss)\n",
    "def compute_loss(predictions, targets):\n",
    "    loss = -np.mean(np.sum(targets * np.log(predictions + EPSILON), axis=1))\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 예측값과 실제 타깃을 비교하여 손실을 계산하는 함수입니다.\n",
    "    - 모델의 예측이 얼마나 틀렸는지 확인하고자 사용하였습니다.\n",
    "- 신경망의 학습 과정에서 오차를 측정하고, 가중치 업데이트를 위한 그레이디언트 계산의 기반이 됩다\n",
    "- 다중 클래스 분류에서 일반적으로 사용하는 크로스 엔트로피 손실을 사용합니다.\n",
    "    - 분류 문제에서 적합:\n",
    "        - 이 손실 함수는 확률 분포 간의 차이를 측정하는 데 사용됩니다.\n",
    "    - Softmax 출력과 자연스럽게 연계:\n",
    "        - 분류 문제에서 출력층에 Softmax 활성화 함수를 사용하는 경우, Cross-Entropy Loss가 최적화에 매우 적합하고, Softmax는 각 클래스에 대한 확률 분포를 생성하므로, 이를 정답 분포와 비교하는 Cross-Entropy Loss가 잘 어울리기에 다음과 같이 설정하였습니다.\n",
    "- EPSILON을 더하여 로그 함수의 0으로 나누는 오류를 방지합니다.\n",
    "- 각 샘플의 손실을 평균하여 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 역전파 함수\n",
    "def backward(predictions, targets, caches):\n",
    "    grads_w = []\n",
    "    grads_b = []\n",
    "\n",
    "    # 출력층 그레이디언트\n",
    "    delta = (predictions - targets) / targets.shape[0]\n",
    "    out_prev, w, b, _ = caches[-1]\n",
    "    dw = out_prev.T @ delta + L2_LAMBDA * w\n",
    "    db = np.sum(delta, axis=0, keepdims=True)\n",
    "    grads_w.insert(0, dw)\n",
    "    grads_b.insert(0, db)\n",
    "    delta = delta @ w.T\n",
    "\n",
    "    # 은닉층 그레이디언트\n",
    "    for i in range(HIDDEN_LAYERS - 1, -1, -1):\n",
    "        out_prev, w, b, z = caches[i]\n",
    "        da = delta * leaky_relu_derivative(z)\n",
    "        dw = out_prev.T @ da + L2_LAMBDA * w\n",
    "        db = np.sum(da, axis=0, keepdims=True)\n",
    "        delta = da @ w.T\n",
    "\n",
    "        grads_w.insert(0, dw)\n",
    "        grads_b.insert(0, db)\n",
    "    return grads_w, grads_b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 역전파를 수행하여 가중치와 편향의 그레이디언트를 계산하는 함수입니다**.**\n",
    "- grads_w, grads_b 리스트는 각 층의 가중치와 편향에 대한 그레이디언트를 저장합니다.\n",
    "- 출력층부터 역으로 계산합니다.\n",
    "    - 출력층의 오차 delta를 계산합니다: delta = (predictions - targets) / targets.shape[0]\n",
    "    - 가중치의 그레이디언트 dw는 이전 층의 출력과 delta의 곱입니다.\n",
    "    - 편향의 그레이디언트 db는 delta의 합입니다.\n",
    "    - L2 정규화를 위해 L2_LAMBDA * w를 더해줍니다.\n",
    "    - delta를 업데이트하여 다음 층으로 전파합니다.\n",
    "- 은닉층에서도 같은 방식으로 계산하되, 활성화 함수의 도함수를 곱해줍니다.\n",
    "- 최종적으로 각 층의 그레이디언트를 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 평가 함수\n",
    "def evaluate(weights, biases, test_data, test_labels, verbose=True):\n",
    "    correct = 0\n",
    "    total_loss = 0\n",
    "    tp = np.zeros(NUM_CLASSES)  # True Positives\n",
    "    fp = np.zeros(NUM_CLASSES)  # False Positives\n",
    "    fn = np.zeros(NUM_CLASSES)  # False Negatives\n",
    "\n",
    "    confusion_matrix = np.zeros((NUM_CLASSES, NUM_CLASSES), dtype=int)\n",
    "\n",
    "    for i in range(test_data.shape[0]):\n",
    "        input_sample = test_data[i:i+1]\n",
    "        target_sample = np.eye(NUM_CLASSES)[test_labels[i:i+1]]\n",
    "        predictions, _ = forward(weights, biases, input_sample)\n",
    "        prediction = np.argmax(predictions, axis=1)[0]\n",
    "        actual = test_labels[i]\n",
    "        total_loss += compute_loss(predictions, target_sample)\n",
    "\n",
    "        confusion_matrix[actual, prediction] += 1\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Sample {i + 1}: Predicted = {animal_names[prediction]}, Actual = {animal_names[actual]}\")\n",
    "        \n",
    "        if prediction == actual:\n",
    "            correct += 1\n",
    "            tp[actual] += 1\n",
    "        else:\n",
    "            fp[prediction] += 1\n",
    "            fn[actual] += 1\n",
    "\n",
    "    accuracy = correct / test_data.shape[0]\n",
    "\n",
    "    # 클래스별 정밀도 및 재현율 계산\n",
    "    precision_per_class = np.zeros(NUM_CLASSES)\n",
    "    recall_per_class = np.zeros(NUM_CLASSES)\n",
    "    for i in range(NUM_CLASSES):\n",
    "        if tp[i] + fp[i] > 0:\n",
    "            precision_per_class[i] = tp[i] / (tp[i] + fp[i])\n",
    "        else:\n",
    "            precision_per_class[i] = 0.0\n",
    "        if tp[i] + fn[i] > 0:\n",
    "            recall_per_class[i] = tp[i] / (tp[i] + fn[i])\n",
    "        else:\n",
    "            recall_per_class[i] = 0.0\n",
    "\n",
    "    # 평균 정밀도 및 재현율 계산\n",
    "    average_precision = np.mean(precision_per_class)\n",
    "    average_recall = np.mean(recall_per_class)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\\n\")\n",
    "    for i in range(NUM_CLASSES):\n",
    "        print(f\"Class '{animal_names[i]}': Precision = {precision_per_class[i] * 100:.2f}%, Recall = {recall_per_class[i] * 100:.2f}%\")\n",
    "    print(f\"\\nAverage Precision: {average_precision * 100:.2f}%\")\n",
    "    print(f\"Average Recall: {average_recall * 100:.2f}%\\n\")\n",
    "\n",
    "    # Confusion Matrix 출력\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(\"Actual \\\\ Predicted\")\n",
    "    print(\"          \", end=\"\")\n",
    "    for name in animal_names:\n",
    "        print(f\"{name:^10}\", end=\"\")\n",
    "    print()\n",
    "    for i, row in enumerate(confusion_matrix):\n",
    "        print(f\"{animal_names[i]:<10}\", end=\"\")\n",
    "        for val in row:\n",
    "            print(f\"{val:^10}\", end=\"\")\n",
    "        print()\n",
    "\n",
    "    return accuracy, total_loss / test_data.shape[0], precision_per_class, average_precision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모델의 성능을 평가하기 위한 함수입니다.\n",
    "- 정확도, 정밀도, 재현율, 혼동 행렬 등을 계산합니다.\n",
    "- 각 샘플에 대해 예측값과 실제 값을 비교하고, 혼동 행렬을 업데이트합니다.\n",
    "- 클래스별로 정밀도와 재현율을 계산하고 출력합니다.\n",
    "- 평균 정밀도와 평균 재현율을 계산합니다.\n",
    "- 최종적으로 정확도, 평균 손실, 클래스별 정밀도, 평균 정밀도를 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_weights_and_biases(weights, biases, weight_file=r\"C:\\weights\\weights.bin\", bias_file=r\"C:\\biases\\biases.bin\"):\n",
    "    # 가중치 저장\n",
    "    with open(weight_file, 'wb') as wf:\n",
    "        for w in weights:\n",
    "            w.astype(np.float32).tofile(wf)  # 이진 형식으로 저장\n",
    "\n",
    "    # 편향 저장\n",
    "    with open(bias_file, 'wb') as bf:\n",
    "        for b in biases:\n",
    "            b.astype(np.float32).tofile(bf)  # 이진 형식으로 저장\n",
    "    \n",
    "    print(f\"Weights saved to {weight_file}\")\n",
    "    print(f\"Biases saved to {bias_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 학습된 가중치와 편향을 파일로 저장하는 함수로 c코드에서 활용하기 위해 추가한 함수입니다.\n",
    "- 가중치와 편향을 이진 형식으로 저장하여 저장하고자하는 경로를 지정한 뒤에 이후에 로드할 수 있도록 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 메인 학습 루프\n",
    "def main():\n",
    "    train_directory = r\"C:\\Users\\최승렬\\Desktop\\AI\\augmented_images\\train_pgm\"\n",
    "    test_directory = r\"C:\\Users\\최승렬\\Desktop\\AI\\augmented_images\\test_pgm\"\n",
    "\n",
    "    train_filenames = [f for f in os.listdir(train_directory) if f.endswith(\".pgm\")]\n",
    "    test_filenames = [f for f in os.listdir(test_directory) if f.endswith(\".pgm\")]\n",
    "\n",
    "    train_data, train_labels = load_images_with_labels(train_directory, train_filenames)\n",
    "    test_data, test_labels = load_images_with_labels(test_directory, test_filenames)\n",
    "\n",
    "    # 라벨이 -1인 경우(라벨을 찾지 못한 경우) 제거\n",
    "    valid_indices = train_labels != -1\n",
    "    train_data = train_data[valid_indices]\n",
    "    train_labels = train_labels[valid_indices]\n",
    "\n",
    "    valid_indices = test_labels != -1\n",
    "    test_data = test_data[valid_indices]\n",
    "    test_labels = test_labels[valid_indices]\n",
    "\n",
    "    # 데이터 및 라벨 확인\n",
    "    for idx in range(10):\n",
    "        print(f\"Filename: {train_filenames[idx]}, Label: {train_labels[idx]}, Class: {animal_names[train_labels[idx]]}\")\n",
    "\n",
    "    unique, counts = np.unique(train_labels, return_counts=True)\n",
    "    print(\"Training data class distribution:\", dict(zip(unique, counts)))\n",
    "\n",
    "    unique, counts = np.unique(test_labels, return_counts=True)\n",
    "    print(\"Test data class distribution:\", dict(zip(unique, counts)))\n",
    "\n",
    "    # 데이터 정규화\n",
    "    mean = np.mean(train_data, axis=0)\n",
    "    std = np.std(train_data, axis=0) + EPSILON\n",
    "    train_data = (train_data - mean) / std\n",
    "    test_data = (test_data - mean) / std\n",
    "\n",
    "    np.save('C:\\weights\\mean.npy', mean)\n",
    "    np.save('C:\\weights\\std.npy', std)\n",
    "    \n",
    "    mean.astype(np.float64).tofile('C:\\weights\\mean.bin')\n",
    "    std.astype(np.float64).tofile('C:\\weights\\std.bin') \n",
    "    \n",
    "    weights, biases = init_network()\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    # Adam 옵티마이저 파라미터 초기화\n",
    "    m_w = [np.zeros_like(w) for w in weights]\n",
    "    v_w = [np.zeros_like(w) for w in weights]\n",
    "    m_b = [np.zeros_like(b) for b in biases]\n",
    "    v_b = [np.zeros_like(b) for b in biases]\n",
    "    beta1 = 0.5\n",
    "    beta2 = 0.9\n",
    "    epsilon = EPSILON\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        learning_rate = INITIAL_LEARNING_RATE\n",
    "        print(f\"\\nEpoch {epoch + 1}/{EPOCHS} - Learning Rate: {learning_rate:.6f}\")\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        # 셔플 데이터\n",
    "        indices = np.arange(train_data.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "        train_data = train_data[indices]\n",
    "        train_labels = train_labels[indices]\n",
    "\n",
    "        for step in range(0, len(train_data), BATCH_SIZE):\n",
    "            input_batch = train_data[step:step + BATCH_SIZE]\n",
    "            target_batch_indices = train_labels[step:step + BATCH_SIZE]\n",
    "            target_batch = np.eye(NUM_CLASSES)[target_batch_indices]\n",
    "\n",
    "            # 순전파\n",
    "            predictions, caches = forward(weights, biases, input_batch)\n",
    "\n",
    "            # 손실 계산\n",
    "            batch_loss = compute_loss(predictions, target_batch)\n",
    "            epoch_loss += batch_loss\n",
    "\n",
    "            # 역전파\n",
    "            grads_w, grads_b = backward(predictions, target_batch, caches)\n",
    "\n",
    "            # 그레이디언트 클리핑 적용\n",
    "            for i in range(len(grads_w)):\n",
    "                grad_norm_w = np.linalg.norm(grads_w[i])\n",
    "                if grad_norm_w > MAX_GRAD_NORM:\n",
    "                    grads_w[i] = grads_w[i] * (MAX_GRAD_NORM / grad_norm_w)\n",
    "                grad_norm_b = np.linalg.norm(grads_b[i])\n",
    "                if grad_norm_b > MAX_GRAD_NORM:\n",
    "                    grads_b[i] = grads_b[i] * (MAX_GRAD_NORM / grad_norm_b)\n",
    "\n",
    "            # 그레이디언트 노름 출력\n",
    "            grad_norm = np.linalg.norm(grads_w[-1])\n",
    "            print(f\"Epoch {epoch + 1}, Batch {step // BATCH_SIZE + 1}: Grad Norm = {grad_norm:.6f}\")\n",
    "\n",
    "            # Adam 옵티마이저를 사용한 가중치 및 편향 업데이트\n",
    "            for i in range(len(weights)):\n",
    "                # 모멘트 업데이트\n",
    "                m_w[i] = beta1 * m_w[i] + (1 - beta1) * grads_w[i]\n",
    "                v_w[i] = beta2 * v_w[i] + (1 - beta2) * (grads_w[i] ** 2)\n",
    "                m_b[i] = beta1 * m_b[i] + (1 - beta1) * grads_b[i]\n",
    "                v_b[i] = beta2 * v_b[i] + (1 - beta2) * (grads_b[i] ** 2)\n",
    "                \n",
    "                # 모멘트 편향 보정\n",
    "                m_w_hat = m_w[i] / (1 - beta1 ** (epoch + 1))\n",
    "                v_w_hat = v_w[i] / (1 - beta2 ** (epoch + 1))\n",
    "                m_b_hat = m_b[i] / (1 - beta1 ** (epoch + 1))\n",
    "                v_b_hat = v_b[i] / (1 - beta2 ** (epoch + 1))\n",
    "                \n",
    "                # 가중치 및 편향 업데이트\n",
    "                weights[i] -= learning_rate * m_w_hat / (np.sqrt(v_w_hat) + epsilon)\n",
    "                biases[i] -= learning_rate * m_b_hat / (np.sqrt(v_b_hat) + epsilon)\n",
    "\n",
    "        train_losses.append(epoch_loss / (len(train_data) // BATCH_SIZE))\n",
    "        _, val_loss, _, _ = evaluate(weights, biases, test_data, test_labels, verbose=False)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1} - Training Loss: {train_losses[-1]:.4f}, Validation Loss: {val_losses[-1]:.4f}\")\n",
    "\n",
    "        # 일부 샘플에 대한 출력 확률 분포 확인\n",
    "        sample_output, _ = forward(weights, biases, test_data[0:5])\n",
    "        print(f\"Sample Output Probabilities (First 5 Samples):\\n{sample_output}\")\n",
    "\n",
    "    save_weights_and_biases(weights, biases)\n",
    "        \n",
    "    # 학습 및 검증 손실 그래프 시각화\n",
    "    plt.plot(range(EPOCHS), train_losses, label='Training Loss')\n",
    "    plt.plot(range(EPOCHS), val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss Over Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nFinal evaluation:\")\n",
    "    evaluate(weights, biases, test_data, test_labels, verbose=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전체 학습 프로세스를 관리하는 메인 함수로, 데이터 로드, 전처리, 모델 초기화, 학습 루프, 평가 를 수행합니다.\n",
    "\n",
    "- 학습 및 테스트 데이터셋 디렉토리를 지정하고, 파일명을 가져옵니다. 여기서 생성된 데이터셋의 경로를 지정해서 실험을 진행하면 됩니다.\n",
    "- load_images_with_labels 함수를 사용하여 데이터를 로드합니다.\n",
    "- 라벨을 찾지 못한 (-1) 데이터 샘플을 제거합니다.\n",
    "- 일부 데이터 샘플의 파일명과 라벨을 출력하여 확인합니다.\n",
    "- 클래스별 데이터 분포를 출력합니다.\n",
    "- 데이터의 평균과 표준편차를 계산하여 표준화합니다.\n",
    "- 학습 데이터의 평균과 표준편차를 사용하여 테스트 데이터도 동일하게 정규화합니다.\n",
    "- np.save 사용하여mean과 std를 Numpy의 바이너리 형식으로 저장하였습니다.\n",
    "- tofile 사용하여 mean과 std를 .bin 파일로 저장. 이진 데이터로 저장하며, 저장 전에 float64 타입으로 변환하였고, 이를 C코드에서 정규화 데이터를 사용할 때 활용합니다. 각각 저장하고자 하는 경로를 지정해주면 됩니다.\n",
    "- 신경망의 가중치와 편향을 초기화합니다.\n",
    "- 에폭별로 손실 값을 저장하기 위한 리스트를 초기화합니다.\n",
    "- 에폭 수만큼 반복하여 학습합니다.\n",
    "- 각 에폭마다 학습률을 설정하고, 에폭 손실을 초기화합니다.\n",
    "- 데이터를 셔플하여 미니배치가 다양하게 구성되도록 합니다.\n",
    "- 미니배치 단위로 데이터를 처리합니다.\n",
    "- 입력 배치를 모델에 통과시켜 예측값을 얻습니다.\n",
    "- 예측값과 실제 타깃을 비교하여 손실을 계산하고, 에폭 손실에 누적합니다.\n",
    "- 역전파를 수행하여 그레이디언트를 계산합니다.\n",
    "- 그레이디언트의 노름이 MAX_GRAD_NORM을 넘을 경우 스케일링하여 클리핑합니다.\n",
    "- 마지막 층의 가중치 그레이디언트 노름을 계산하여 출력합니다.\n",
    "- 학습 과정 중 그레이디언트의 변화를 모니터링할 수 있습니다.\n",
    "- Adam 옵티마이저의 업데이트 규칙을 적용하여 가중치와 편향을 업데이트합니다.\n",
    "- 1차 모멘트(m_w, m_b)와 2차 모멘트(v_w, v_b)를 업데이트합니다.\n",
    "- 모멘트 편향 보정을 수행하여 초기 단계의 편향을 제거합니다.\n",
    "- 학습률과 함께 가중치와 편향을 업데이트합니다.\n",
    "- 에폭이 끝날 때마다 평균 손실을 계산하여 저장합니다.\n",
    "- evaluate 함수를 사용하여 검증 데이터에 대한 손실을 계산합니다.\n",
    "- 학습 손실과 검증 손실을 출력하여 학습 과정을 모니터링하고자 했습니다.\n",
    "- 테스트 데이터의 첫 5개 샘플에 대한 예측 확률 분포를 출력합니다.\n",
    "- 모델이 어떻게 예측하고 있는지 확인하고자 했습니다.\n",
    "- 학습이 완료된 후 가중치와 편향을 저장해서 후에 c코드에서 평가할 때 사용합니다.\n",
    "- 에폭별 학습 손실과 검증 손실을 그래프로 시각화합니다.\n",
    "- 학습 과정 중 손실의 변화를 시각적으로 확인할 수 있습니다.\n",
    "- 전체 테스트 데이터에 대해 최종 평가를 수행하고, 자세한 결과를 출력합니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
